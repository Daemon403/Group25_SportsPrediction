# -*- coding: utf-8 -*-
"""Group25_SportsPrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AFF-apNCQA_W2HQXf7uKZxOWXgSyxvPr
"""

!pip install --upgrade scikit-learn
import pandas as pd
import csv
import numpy as np
from google.colab import drive
drive.mount('/content/drive')

"""Dropping colums"""

data21 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MidSemProject/players_21.csv')
data22 = pd.read_csv('/content/drive/My Drive/Colab Notebooks/MidSemProject/players_22.csv')

new_data_21 = data21.drop(['sofifa_id','player_url','short_name','long_name','dob','club_team_id','club_jersey_number','club_loaned_from','club_joined','club_contract_valid_until','nationality_id','nation_team_id','nation_jersey_number','real_face','player_face_url','club_logo_url','club_flag_url','nation_logo_url','nation_flag_url'], axis=1)
new_data_22 = data22.drop(['sofifa_id','player_url','short_name','long_name','dob','club_team_id','club_jersey_number','club_loaned_from','club_joined','club_contract_valid_until','nationality_id','nation_team_id','nation_jersey_number','real_face','player_face_url','club_logo_url','club_flag_url','nation_logo_url','nation_flag_url'], axis=1)

"""Dropping columns that had more than 30% of data being null values"""

threshold = len(new_data_22) * 0.3
new_data_22 = new_data_22.dropna(thresh=threshold, axis=1)

threshold = len(new_data_21) * 0.3
new_data_22 = new_data_21.dropna(thresh=threshold, axis=1)

"""Imputing data using forward and backwards fill"""

new_data_21 =new_data_21.ffill()
new_data_21 = new_data_21.bfill()

new_data_22 =new_data_22.ffill()
new_data_22 = new_data_22.bfill()

"""Creating subsets based on value types (categorical vs numerical)"""

#Numerical dataset (num)
num = new_data_21.select_dtypes(['int64','float64']).columns
num_data = new_data_21[num]

num22 = new_data_22.select_dtypes(['int64','float64']).columns
num_data22 = new_data_22[num22]

num

#categorical dataset (cat)
cat = new_data_21.select_dtypes(include =['object']).columns
cat_data = new_data_21[cat]

cat22 = new_data_22.select_dtypes(include =['object']).columns
cat_data22 = new_data_22[cat22]

cat

num_data

cat_data

"""Encoding using factorize"""

cat_data_fact = cat_data.apply(lambda x: pd.factorize(x)[0])
cat_data_fact22 = cat_data22.apply(lambda x: pd.factorize(x)[0])

cat_data_fact

"""Since we now have only numerical data, we can safely concaternate and use accodingly"""

new_data21 = pd.concat([num_data, cat_data_fact], axis = 1)
new_data22 = pd.concat([num_data22, cat_data_fact22], axis = 1)

new_data21.info()

"""Our target colum is 'overall'

however, we also drop the 'potential' column as it is a forecast or prediction of the level which a player can reach
"""

target = new_data21['overall']
target22 = new_data22['overall']
Y = target
features = new_data21.drop(['overall','potential'], axis=1)
features.info()

target.info()

"""Feature selection using correlation. 21 features were selected and used for training"""

features = new_data21.corrwith(target).abs().index

features= new_data21[features[2:23]]
features22 = new_data22[features.columns]
new_data22

features

"""Listing out the column names to understand the parameters being used"""

features.columns

Y = Y.to_frame()
Y = Y['overall']
Y22 = new_data_22['overall']

"""Scaling and spliting data"""

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

X = features
X22 = features22
X=StandardScaler().fit_transform(X.copy())
X22 =StandardScaler().fit_transform(X22.copy())
xtrain, xtest, ytrain, ytest = train_test_split(X, Y, test_size=0.2, random_state=42)
ytest = Y22
xtest = X22

"""Training XGBoost model"""

from xgboost import XGBRegressor
xgb = XGBRegressor(n_estimators=42)
xgb.fit(xtrain, ytrain)

"""Training SVR model"""

from sklearn.svm import SVR
sv = SVR(kernel = 'rbf')
sv.fit(xtrain,ytrain)

"""Training RandomForestRegressor model"""

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(n_estimators=17,random_state=43)
regressor.fit(xtrain, ytrain)

"""Training Voting Regressor"""

from sklearn.ensemble import VotingRegressor
from sklearn.metrics import accuracy_score
voting_regressor = VotingRegressor(estimators=[
    ('xgb', xgb),
    ('sv', sv),
    ('regressor', regressor)
])
voting_regressor.fit(xtrain,ytrain)

"""Cross validation using KFold and cross_val_score"""

from sklearn.model_selection import KFold, cross_val_score
k_folds = KFold(n_splits = 5)

scores_xgb = cross_val_score(xgb, xtrain, ytrain, cv = k_folds)
scores_sv = cross_val_score(sv, xtrain, ytrain, cv = k_folds)
scores_reg = cross_val_score(regressor, xtrain, ytrain, cv = k_folds)
scores_voting = cross_val_score(voting_regressor, xtrain, ytrain, cv = k_folds)

print(scores_xgb.mean())
print(scores_sv.mean())
print(scores_reg.mean())
print(scores_voting.mean())

"""Perfromance analysis using MAE, MSE,R2"""

from sklearn.metrics import mean_absolute_error
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score

"""CHECKING RESULTS FROM EACH MODEL AND COMPARING TO DETERMINE THE BEST TO USE"""

y_pred_xgb = xgb.predict(xtest)
mae = mean_absolute_error(ytest, y_pred_xgb)
mse = mean_squared_error(ytest, y_pred_xgb)
rmse = np.sqrt(mse)
r2 = r2_score(ytest, y_pred_xgb)

print("The XGB performance for testing set")
print("-------------------------------------")
print('MAE is {}'.format(mae))
print('MSE is {}'.format(mse))
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))

y_pred_sv = sv.predict(xtest)
mae = mean_absolute_error(ytest, y_pred_sv)
mse = mean_squared_error(ytest, y_pred_sv)
rmse = np.sqrt(mse)
r2 = r2_score(ytest, y_pred_sv)

print("The SV performance for testing set")
print("-------------------------------------")
print('MAE is {}'.format(mae))
print('MSE is {}'.format(mse))
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))

y_pred_random = regressor.predict(xtest)
mae = mean_absolute_error(ytest, y_pred_random)
mse = mean_squared_error(ytest, y_pred_random)
rmse = np.sqrt(mse)
r2 = r2_score(ytest, y_pred_random)

print("The RANDOM FOREST performance for testing set")
print("-------------------------------------")
print('MAE is {}'.format(mae))
print('MSE is {}'.format(mse))
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))

y_pred_voting = voting_regressor.predict(xtest)
mae = mean_absolute_error(ytest, y_pred_voting)
mse = mean_squared_error(ytest, y_pred_voting)
rmse = np.sqrt(mse)
r2 = r2_score(ytest, y_pred_voting)

print("The VOTING performance for testing set")
print("-------------------------------------")
print('MAE is {}'.format(mae))
print('MSE is {}'.format(mse))
print('R2 score is {}'.format(r2))

"""IMPROVING THE MODEL USING ADA BOOST

The ada model was fine tuned using the AdaBoostRegressor to improve performance
"""

from sklearn.ensemble import AdaBoostRegressor
ada  = AdaBoostRegressor(base_estimator=regressor, n_estimators= 4, learning_rate = 0.001)

ada.fit(xtrain,ytrain)

y_pred_ada = ada.predict(xtest)
mae = mean_absolute_error(ytest, y_pred_ada)
mse = mean_squared_error(ytest, y_pred_ada)
rmse = np.sqrt(mse)
r2 = r2_score(ytest, y_pred_ada)

print("The ADA performance for testing set")
print("-------------------------------------")
print('MAE is {}'.format(mae))
print('MSE is {}'.format(mse))
print('RMSE is {}'.format(rmse))
print('R2 score is {}'.format(r2))

"""Testing the improved model"""

ada_predict = ada.predict(xtest)
accuracy = ((ada_predict-ytest)/ytest)*100
accuracy

import pickle
model = ada
with open('/content/drive/My Drive/Colab Notebooks/MidSemProject/adamodel.pkl','wb') as f:
  pickle.dump(model,f)

from joblib import dump, load
with open('/content/drive/My Drive/Colab Notebooks/MidSemProject/adamodelfinal.joblib','wb') as f:
  dump(model,f)